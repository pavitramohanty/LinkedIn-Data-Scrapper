{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\\ncollege_name = edu_section.find('h3').get_text().strip()\\ndegree_name = edu_section.find('p', {'class': 'pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal'}).find_all('span')[1].get_text().strip()\\nstream = edu_section.find('p', {'class': 'pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal'}).find_all('span')[1].get_text().strip()\\ndegree_year = edu_section.find('p', {'class': 'pv-entity__dates t-14 t-black--light t-normal'}).find_all('span')[1].get_text().strip()\\ninfo.append(college_name)\\ninfo.append(degree_name)\\ninfo.append(stream)\\ninfo.append(degree_year)\\ninfo\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, time, random\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "browser = webdriver.Chrome('C:/Users/Admin/Desktop/LinkedIn-Profile-Scrapper-in-Python-master/chromedriver.exe')\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "file = open('config.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "\n",
    "elementID.submit()\n",
    "\n",
    "browser = webdriver.Chrome('C:/Users/Admin/Desktop/LinkedIn-Profile-Scrapper-in-Python-master/chromedriver.exe')\n",
    "browser.get('https://www.linkedin.com/uas/login')\n",
    "file = open('config.txt')\n",
    "lines = file.readlines()\n",
    "username = lines[0]\n",
    "password = lines[1]\n",
    "\n",
    "\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "\n",
    "elementID.submit()\n",
    "\n",
    "link_from_csv=pd.read_csv('C:/Users/Admin/Desktop/final_output.csv')\n",
    "#print(link_from_csv.head())\n",
    "final_list=link_from_csv[1:]\n",
    "profile_list=list()\n",
    "profile_list=final_list['Link'].tolist()\n",
    "#print(profile_list)\n",
    "#link = 'https://www.linkedin.com/in/mayank-agrawal-1a5883148/'\n",
    "for link in profile_list:\n",
    "    browser = webdriver.Chrome('C:/Users/Admin/Desktop/LinkedIn-Profile-Scrapper-in-Python-master/chromedriver.exe')\n",
    "    browser.get('https://www.linkedin.com/uas/login')\n",
    "    file = open('config.txt')\n",
    "    lines = file.readlines()\n",
    "    username = lines[0]\n",
    "    password = lines[1]\n",
    "\n",
    "    elementID = browser.find_element_by_id('username')\n",
    "    elementID.send_keys(username)\n",
    "\n",
    "    elementID = browser.find_element_by_id('password')\n",
    "    elementID.send_keys(password)\n",
    "\n",
    "    elementID.submit()\n",
    "    browser.get(link)\n",
    "\n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "# Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for i in range(3):\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    data_dict={}\n",
    "    name=[]\n",
    "    profile_title=[]\n",
    "    loc=[]\n",
    "    connection=[]\n",
    "    link1=[]\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    name_div = soup.find('div', {'class': 'flex-1 mr5'})\n",
    "    name_loc = name_div.find_all('ul')\n",
    "    name1 = name_loc[0].find('li').get_text().strip()\n",
    "    loc1 = name_loc[1].find('li').get_text().strip()\n",
    "    profile_title1 = name_div.find('h2').get_text().strip()\n",
    "    connection1 = name_loc[1].find_all('li')\n",
    "    connection1 = connection1[1].get_text().strip()\n",
    "    link=link1.append(link)\n",
    "    name=name.append(name1)\n",
    "    profile_title=profile_title.append(profile_title1)\n",
    "    loc=loc.append(loc1)\n",
    "    connection=connection.append(connection1)\n",
    "dict={\n",
    "    \"Name\":name,\n",
    "    \"Profile_title\":profile_title,\n",
    "    \"Location\":loc,\n",
    "    \"Number of Connection\":connection,\n",
    "    \"Profile Link\":link1\n",
    "}\n",
    "a=pd.DataFrame(data_dict)    \n",
    "print(a)\n",
    "'''info\n",
    "    exp_section = soup.find('section', {'id': 'experience-section'})\n",
    "    exp_section = exp_section.find('ul')\n",
    "    new_data=soup.find_all('h3',{'class':'t-16 t-black t-bold'})\n",
    "    li_tags = exp_section.find('div')\n",
    "    a_tags = li_tags.find('a')\n",
    "    job_title = a_tags.find('h3').get_text().strip()\n",
    "    company_name = a_tags.find_all('p')[1].get_text().strip()\n",
    "    joining_date = a_tags.find_all('h4')[0].find_all('span')[1].get_text().strip()\n",
    "    exp = a_tags.find_all('h4')[1].find_all('span')[1].get_text().strip()\n",
    "    info.append(company_name)\n",
    "    info.append(job_title)\n",
    "    info.append(joining_date)\n",
    "    info.append(exp)\n",
    "    print(info)\n",
    "    browser.close()\n",
    "'''\n",
    "'''edu_section = soup.find('section', {'id': 'education-section'}).find('ul')\n",
    "college_name = edu_section.find('h3').get_text().strip()\n",
    "degree_name = edu_section.find('p', {'class': 'pv-entity__secondary-title pv-entity__degree-name t-14 t-black t-normal'}).find_all('span')[1].get_text().strip()\n",
    "stream = edu_section.find('p', {'class': 'pv-entity__secondary-title pv-entity__fos t-14 t-black t-normal'}).find_all('span')[1].get_text().strip()\n",
    "degree_year = edu_section.find('p', {'class': 'pv-entity__dates t-14 t-black--light t-normal'}).find_all('span')[1].get_text().strip()\n",
    "info.append(college_name)\n",
    "info.append(degree_name)\n",
    "info.append(stream)\n",
    "info.append(degree_year)\n",
    "info\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smarth Kukreja']\n"
     ]
    }
   ],
   "source": [
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
